{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytest\n",
    "import ipytest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, lower, when, to_date, round,\n",
    "    add_months, max as spark_max, split, explode\n",
    ")\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Initialize ipytest for Jupyter\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a46171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Spark Session Management (Spark 2.4.5 SAFE)\n",
    "# ============================================================\n",
    "def get_spark():\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(\"NYC_Job_DE_Challenge\")\n",
    "        .master(\"spark://master:7077\")\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7054323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. Transformation Functions\n",
    "# ============================================================\n",
    "def normalize_columns(df: DataFrame) -> DataFrame:\n",
    "    new_cols = [\n",
    "        c.lower().replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "        for c in df.columns\n",
    "    ]\n",
    "    return df.toDF(*new_cols)\n",
    "\n",
    "def clean_data(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df.dropDuplicates()\n",
    "        .withColumn(\"posting_date\", to_date(\"posting_date\"))\n",
    "        .withColumn(\"post_until\", to_date(\"post_until\"))\n",
    "        .filter(col(\"posting_date\").isNotNull())\n",
    "        .filter(col(\"salary_range_from\").isNotNull())\n",
    "        .filter(col(\"salary_range_to\").isNotNull())\n",
    "        .filter(col(\"salary_range_to\") >= col(\"salary_range_from\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859eb3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d752c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_feature_engineering(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df.withColumn(\n",
    "            \"avg_salary\",\n",
    "            (col(\"salary_range_from\") + col(\"salary_range_to\")) / 2\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"education_level\",\n",
    "            when(lower(col(\"minimum_qual_requirements\")).contains(\"master\"), \"Masters\")\n",
    "            .when(lower(col(\"minimum_qual_requirements\")).contains(\"bachelor\"), \"Bachelors\")\n",
    "            .otherwise(\"Other\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"edu_rank\",\n",
    "            when(col(\"education_level\") == \"Bachelors\", 3)\n",
    "            .when(col(\"education_level\") == \"Masters\", 4)\n",
    "            .otherwise(0)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ce0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 3. KPI Analytics 
    "# ============================================================\n",
    "def compute_kpis(df: DataFrame):\n",
    "    df.createOrReplaceTempView(\"jobs\")\n",
    "\n",
    "  \n",
    "    spark = df.sql_ctx.sparkSession\n",
    "\n",
    "    max_date_val = df.select(\n",
    "        spark_max(\"posting_date\")\n",
    "    ).collect()[0][0]\n",
    "\n",
    "    stats = spark.sql(\"\"\"\n",
    "        SELECT job_category,\n",
    "               COUNT(*) AS job_count,\n",
    "               ROUND(AVG(avg_salary), 2) AS avg_sal\n",
    "        FROM jobs\n",
    "        GROUP BY job_category\n",
    "        ORDER BY job_count DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "\n",
    "    correlation = spark.sql(\"\"\"\n",
    "        SELECT corr(edu_rank, avg_salary) AS edu_salary_corr\n",
    "        FROM jobs\n",
    "        WHERE edu_rank > 0\n",
    "    \"\"\")\n",
    "\n",
    "    highest_sal_agency = spark.sql(\"\"\"\n",
    "        SELECT agency, business_title, avg_salary\n",
    "        FROM (\n",
    "            SELECT agency,\n",
    "                   business_title,\n",
    "                   avg_salary,\n",
    "                   ROW_NUMBER() OVER (\n",
    "                       PARTITION BY agency\n",
    "                       ORDER BY avg_salary DESC\n",
    "                   ) AS rnk\n",
    "            FROM jobs\n",
    "        )\n",
    "        WHERE rnk = 1\n",
    "        ORDER BY avg_salary DESC\n",
    "    \"\"\")\n",
    "\n",
    "    avg_sal_2yr = spark.sql(f\"\"\"\n",
    "        SELECT agency,\n",
    "               ROUND(AVG(avg_salary), 2) AS rolling_avg\n",
    "        FROM jobs\n",
    "        WHERE posting_date >= add_months(\n",
    "            CAST('{max_date_val}' AS DATE), -24\n",
    "        )\n",
    "        GROUP BY agency\n",
    "        ORDER BY rolling_avg DESC\n",
    "    \"\"\")\n",
    "\n",
    "    top_skills = spark.sql(\"\"\"\n",
    "        SELECT TRIM(skill) AS skill,\n",
    "               ROUND(AVG(avg_salary), 2) AS skill_val\n",
    "        FROM jobs\n",
    "        LATERAL VIEW explode(\n",
    "            split(lower(preferred_skills), ',')\n",
    "        ) t AS skill\n",
    "        WHERE skill <> ''\n",
    "        GROUP BY skill\n",
    "        ORDER BY skill_val DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "\n",
    "    return stats, correlation, highest_sal_agency, avg_sal_2yr, top_skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5. Pipeline Execution\n",
    "# ============================================================\n",
    "spark = None\n",
    "try:\n",
    "    print(\">>> Phase 1: Running Unit Tests...\")\n",
    "    ipytest.run()\n",
    "\n",
    "    spark = get_spark()\n",
    "    print(f\">>> Phase 2: Connected to Master. UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "    path = \"/dataset/nyc-jobs.csv\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"{path} not found. Check volume mapping.\")\n",
    "\n",
    "    raw_df = (\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(path)\n",
    "    )\n",
    "\n",
    "    df = apply_feature_engineering(\n",
    "        clean_data(\n",
    "            normalize_columns(raw_df)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    df.show()\n",
    "\n",
    "    stats, salary_distribution, corr, high_agency, avg_2yr, skills_us = compute_kpis(df)\n",
    "\n",
    "    stats.show(5)\n",
    "    corr.show()\n",
    "    high_agency.show(5)\n",
    "    avg_2yr.show(5)\n",
    "    skills_us.show(5)\n",
    "\n",
    "    pdf = stats.toPandas()\n",
    "    pdf.plot(kind=\"bar\", x=\"job_category\", y=\"job_count\", title=\"NYC Job Categories\")\n",
    "    plt.show()\n",
    "\n",
    "    df.unpersist()\n",
    "    print(\">>> Pipeline Finished Successfully\")\n",
    "\n",
    "finally:\n",
    "    if spark:\n",
    "        print(\">>> Stopping Spark session...\")\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Unit Tests\n",
    "# ============================================================\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark_session():\n",
    "    return get_spark()\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def base_df(spark_session):\n",
    "    data = [\n",
    "        (\"IT\", \"Agency1\", \"Engineer\", \"2023-01-01\", \"2023-12-31\", 60000, 80000,\n",
    "         \"Bachelor degree required\", \"python, spark\"),\n",
    "        (\"IT\", \"Agency1\", \"Senior Engineer\", \"2024-01-01\", \"2024-12-31\", 90000, 120000,\n",
    "         \"Master degree preferred\", \"spark, kafka\"),\n",
    "        (\"HR\", \"Agency2\", \"HR Manager\", \"2023-06-01\", \"2023-12-31\", 50000, 70000,\n",
    "         \"Bachelor degree\", \"communication, leadership\")\n",
    "    ]\n",
    "\n",
    "    cols = [\n",
    "        \"job_category\", \"agency\", \"business_title\",\n",
    "        \"posting_date\", \"post_until\",\n",
    "        \"salary_range_from\", \"salary_range_to\",\n",
    "        \"minimum_qual_requirements\", \"preferred_skills\"\n",
    "    ]\n",
    "\n",
    "    return spark_session.createDataFrame(data, cols)\n",
    "\n",
    "\n",
    "def test_normalize_columns():\n",
    "    spark = get_spark()\n",
    "    df = spark.createDataFrame([(1,)], [\"Job ID\"])\n",
    "    result = normalize_columns(df)\n",
    "    assert \"job_id\" in result.columns\n",
    "\n",
    "\n",
    "def test_clean_data_removes_null_posting_date(base_df):\n",
    "    df = base_df.withColumn(\n",
    "        \"posting_date\",\n",
    "        when(col(\"job_category\") == \"IT\", None).otherwise(col(\"posting_date\"))\n",
    "    )\n",
    "    cleaned = clean_data(df)\n",
    "    assert cleaned.count() == 1\n",
    "\n",
    "\n",
    "\n",
    "def test_feature_engineering_education_level(base_df):\n",
    "    df = apply_feature_engineering(clean_data(base_df))\n",
    "    levels = {r.education_level for r in df.select(\"education_level\").collect()}\n",
    "    assert \"Bachelors\" in levels\n",
    "    assert \"Masters\" in levels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45692da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
