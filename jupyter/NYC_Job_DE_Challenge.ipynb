{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NYC Job Data Engineering Challenge\n",
    "---------------------------------\n",
    "\n",
    "This file contains an end-to-end Spark-based data engineering pipeline\n",
    "built for analyzing NYC job postings data.\n",
    "\n",
    "The focus of this implementation is:\n",
    "- Clean and readable transformation logic\n",
    "- Realistic feature engineering\n",
    "- Business-focused analytics (KPIs)\n",
    "- Strong unit test coverage using pytest + ipytest\n",
    "\n",
    "The code is intentionally written in a clear, explicit style so that\n",
    "a reviewer can easily follow the reasoning and verify correctness.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# Imports\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "# Remove pytest and ipytest imports if not available\n",
    "# import pytest\n",
    "# import ipytest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, lower, when, to_date, add_months, max as spark_max, split, explode, trim, avg, round, regexp_replace, length\n",
    ")\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a46171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Initialize ipytest for Jupyter environments\n",
    "# ============================================================\n",
    "ipytest.autoconfig()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Spark Session Management (Spark 2.4.5 SAFE)\n",
    "# ============================================================\n",
    "def get_spark():\n",
    "    \"\"\"\n",
    "    Creates a SparkSession connected to a standalone Spark cluster.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(\"NYC_Job_DE_Challenge\")\n",
    "        .master(\"spark://master:7077\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7054323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 2. Transformation Functions\n",
    "# ============================================================\n",
    "def normalize_columns(df: DataFrame) -> DataFrame:\n",
    "    new_cols = [\n",
    "        c.lower().replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "        for c in df.columns\n",
    "    ]\n",
    "    return df.toDF(*new_cols)\n",
    "\n",
    "\n",
    "def clean_data(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df.dropDuplicates()\n",
    "        .withColumn(\"posting_date\", to_date(\"posting_date\"))\n",
    "        .withColumn(\"post_until\", to_date(\"post_until\"))\n",
    "        .filter(col(\"posting_date\").isNotNull())\n",
    "        .filter(col(\"salary_range_from\").isNotNull())\n",
    "        .filter(col(\"salary_range_to\").isNotNull())\n",
    "        .filter(col(\"salary_range_to\") >= col(\"salary_range_from\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def apply_feature_engineering(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df.withColumn(\n",
    "            \"avg_salary\",\n",
    "            (col(\"salary_range_from\") + col(\"salary_range_to\")) / 2\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"education_level\",\n",
    "            when(lower(col(\"minimum_qual_requirements\")).contains(\"phd\"), \"PhD\")\n",
    "            .when(lower(col(\"minimum_qual_requirements\")).contains(\"doctorate\"), \"Doctorate\")\n",
    "            .when(lower(col(\"minimum_qual_requirements\")).contains(\"master\"), \"Masters\")\n",
    "            .when(lower(col(\"minimum_qual_requirements\")).contains(\"graduate\"), \"Graduate\")\n",
    "            .when(lower(col(\"minimum_qual_requirements\")).contains(\"bachelor\"), \"Bachelors\")\n",
    "            .when(lower(col(\"minimum_qual_requirements\")).contains(\"associate\"), \"Associates\")\n",
    "            .when(lower(col(\"minimum_qual_requirements\")).contains(\"vocational\"), \"Vocational\")\n",
    "            .when(lower(col(\"minimum_qual_requirements\")).contains(\"high school\"), \"HighSchool\")\n",
    "            .otherwise(\"Other\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"edu_rank\",\n",
    "            when(col(\"education_level\") == \"PhD\", 6)\n",
    "            .when(col(\"education_level\") == \"Doctorate\", 6)\n",
    "            .when(col(\"education_level\") == \"Masters\", 5)\n",
    "            .when(col(\"education_level\") == \"Graduate\", 5)\n",
    "            .when(col(\"education_level\") == \"Bachelors\", 4)\n",
    "            .when(col(\"education_level\") == \"Associates\", 2)\n",
    "            .when(col(\"education_level\") == \"Vocational\", 1)\n",
    "            .when(col(\"education_level\") == \"HighSchool\", 1)\n",
    "            .otherwise(0)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d752c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 3. KPI Analytics\n",
    "# ============================================================\n",
    "def compute_kpis(df: DataFrame):\n",
    "    df.createOrReplaceTempView(\"jobs\")\n",
    "    spark = df.sql_ctx.sparkSession\n",
    "\n",
    "    max_date_val = df.select(spark_max(\"posting_date\")).collect()[0][0]\n",
    "\n",
    "    stats = spark.sql(\"\"\"\n",
    "        SELECT job_category,\n",
    "               COUNT(*) AS job_count,\n",
    "               ROUND(AVG(avg_salary), 2) AS avg_sal\n",
    "        FROM jobs\n",
    "        GROUP BY job_category\n",
    "        ORDER BY job_count DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    salary_distribution = spark.sql(\"\"\"\n",
    "        SELECT job_category,\n",
    "               MIN(avg_salary) AS min_salary,\n",
    "               MAX(avg_salary) AS max_salary,\n",
    "               ROUND(AVG(avg_salary), 2) AS avg_salary,\n",
    "               percentile_approx(avg_salary, 0.5) AS median_salary\n",
    "        FROM jobs\n",
    "        GROUP BY job_category\n",
    "        ORDER BY avg_salary DESC\n",
    "    \"\"\")\n",
    "    correlation = spark.sql(\"\"\"\n",
    "        SELECT corr(edu_rank, avg_salary) AS edu_salary_corr\n",
    "        FROM jobs\n",
    "        WHERE edu_rank > 0\n",
    "    \"\"\")\n",
    "\n",
    "    highest_sal_agency = spark.sql(\"\"\"\n",
    "        SELECT agency, business_title, avg_salary\n",
    "        FROM (\n",
    "            SELECT agency,\n",
    "                   business_title,\n",
    "                   avg_salary,\n",
    "                   ROW_NUMBER() OVER (\n",
    "                       PARTITION BY agency\n",
    "                       ORDER BY avg_salary DESC\n",
    "                   ) AS rnk\n",
    "            FROM jobs\n",
    "        )\n",
    "        WHERE rnk = 1\n",
    "        ORDER BY avg_salary DESC\n",
    "    \"\"\")\n",
    "\n",
    "    avg_sal_2yr = spark.sql(f\"\"\"\n",
    "        SELECT agency,\n",
    "               ROUND(AVG(avg_salary), 2) AS rolling_avg\n",
    "        FROM jobs\n",
    "        WHERE posting_date >= add_months(\n",
    "            CAST('{max_date_val}' AS DATE), -24\n",
    "        )\n",
    "        GROUP BY agency\n",
    "        ORDER BY rolling_avg DESC\n",
    "    \"\"\")\n",
    "\n",
    "    # Robust skill extraction for NYC jobs\n",
    "    stopwords = [\n",
    "        \"and\", \"or\", \"with\", \"experience\", \"years\", \"year\", \"skills\",\n",
    "        \"knowledge\", \"ability\", \"including\", \"etc\", \"preferred\", \"required\",\n",
    "        \"strong\", \"excellent\", \"good\", \"demonstrated\", \"responsibilities\"\n",
    "    ]\n",
    "    nyc_regex = (\n",
    "        r\"(\\bmanhattan\\b|\\bnew york city\\b|\\bnyc\\b|\\bnew york,? ny\\b|\\bnew york\\b|\"\n",
    "        r\"\\bbronx\\b|\\bbrooklyn\\b|\\bqueens\\b|\\bstaten island\\b|\\bny,? ny\\b|\"\n",
    "        r\"\\bny ny\\b|\\bnyc\\b|\\bnew york county\\b|\\bqueens county\\b|\\bbronx county\\b|\"\n",
    "        r\"\\bbrooklyn,? ny\\b|\\bmanhattan,? ny\\b|\\bstaten island,? ny\\b)\"\n",
    "    )\n",
    "    from pyspark.sql.functions import regexp_replace, length\n",
    "    top_skills = (\n",
    "        df.filter(col(\"work_location\").isNotNull())\n",
    "          .filter(lower(col(\"work_location\")).rlike(nyc_regex))\n",
    "          .withColumn(\"skill\", explode(split(lower(col(\"preferred_skills\")), \"[,;/|]+\")))\n",
    "          .withColumn(\"skill\", trim(col(\"skill\")))\n",
    "          .withColumn(\"skill\", regexp_replace(col(\"skill\"), \"[\\u2018\\u2019\\u201c\\u201d\\u2013\\u2014,\\.\\(\\)\\[\\]:]\", \"\"))\n",
    "          .filter(col(\"skill\").isNotNull())\n",
    "          .filter(col(\"skill\") != \"\")\n",
    "          .filter(length(col(\"skill\")) > 1)\n",
    "          .filter(col(\"skill\").rlike(\"^[a-zA-Z].*$\"))\n",
    "          .filter(~col(\"skill\").isin(*stopwords))\n",
    "          .groupBy(\"skill\")\n",
    "          .agg(round(avg(col(\"avg_salary\").cast(\"double\")), 2).alias(\"skill_val\"))\n",
    "          .orderBy(col(\"skill_val\").desc())\n",
    "          .limit(10)\n",
    "    )\n",
    "\n",
    "    return stats,salary_distribution, correlation, highest_sal_agency, avg_sal_2yr, top_skills\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ce0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 5. Pipeline Execution\n",
    "# ============================================================\n",
    "spark = None\n",
    "try:\n",
    "    print(\">>> Phase 1: Running Unit Tests...\")\n",
    "    ipytest.run()\n",
    "\n",
    "    spark = get_spark()\n",
    "    print(f\">>> Phase 2: Connected to Master. UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "    path = \"/dataset/nyc-jobs.csv\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"{path} not found. Check volume mapping.\")\n",
    "\n",
    "    raw_df = (\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(path)\n",
    "    )\n",
    "\n",
    "    df = apply_feature_engineering(\n",
    "        clean_data(\n",
    "            normalize_columns(raw_df)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    df.show()\n",
    "\n",
    "    stats, salary_distribution, corr, high_agency, avg_2yr, skills_us = compute_kpis(df)\n",
    "\n",
    "    stats.show(5)\n",
    "    corr.show()\n",
    "    high_agency.show(5)\n",
    "    avg_2yr.show(5)\n",
    "    skills_us.show(5)\n",
    "\n",
    "    pdf = stats.toPandas()\n",
    "    pdf.plot(kind=\"bar\", x=\"job_category\", y=\"job_count\", title=\"NYC Job Categories\")\n",
    "    plt.show()\n",
    "\n",
    "    df.unpersist()\n",
    "    print(\">>> Pipeline Finished Successfully\")\n",
    "\n",
    "finally:\n",
    "    if spark:\n",
    "        print(\">>> Stopping Spark session...\")\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45692da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. Unit Tests\n",
    "# ============================================================\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark_session():\n",
    "    return get_spark()\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def base_df(spark_session):\n",
    "    data = [\n",
    "        (\"IT\", \"Agency1\", \"Engineer\", \"2023-01-01\", \"2023-12-31\", 60000, 80000,\n",
    "         \"Bachelor degree required\", \"python, spark\"),\n",
    "        (\"IT\", \"Agency1\", \"Senior Engineer\", \"2024-01-01\", \"2024-12-31\", 90000, 120000,\n",
    "         \"Master degree preferred\", \"spark, kafka\"),\n",
    "        (\"HR\", \"Agency2\", \"HR Manager\", \"2023-06-01\", \"2023-12-31\", 50000, 70000,\n",
    "         \"Bachelor degree\", \"communication, leadership\")\n",
    "    ]\n",
    "\n",
    "    cols = [\n",
    "        \"job_category\", \"agency\", \"business_title\",\n",
    "        \"posting_date\", \"post_until\",\n",
    "        \"salary_range_from\", \"salary_range_to\",\n",
    "        \"minimum_qual_requirements\", \"preferred_skills\"\n",
    "    ]\n",
    "\n",
    "    return spark_session.createDataFrame(data, cols)\n",
    "\n",
    "\n",
    "def test_normalize_columns():\n",
    "    spark = get_spark()\n",
    "    df = spark.createDataFrame([(1,)], [\"Job ID\"])\n",
    "    result = normalize_columns(df)\n",
    "    assert \"job_id\" in result.columns\n",
    "\n",
    "\n",
    "def test_clean_data_filters_invalid_salary(base_df):\n",
    "    df = base_df.union(\n",
    "        base_df.limit(1).withColumn(\"salary_range_to\", col(\"salary_range_from\") - 1)\n",
    "    )\n",
    "    cleaned = clean_data(df)\n",
    "    assert cleaned.count() == 3\n",
    "\n",
    "\n",
    "def test_clean_data_removes_null_posting_date(base_df):\n",
    "    df = base_df.withColumn(\n",
    "        \"posting_date\",\n",
    "        when(col(\"job_category\") == \"IT\", None).otherwise(col(\"posting_date\"))\n",
    "    )\n",
    "    cleaned = clean_data(df)\n",
    "    assert cleaned.count() == 1\n",
    "\n",
    "\n",
    "def test_feature_engineering_avg_salary(base_df):\n",
    "    df = apply_feature_engineering(clean_data(base_df))\n",
    "    row = df.filter(col(\"business_title\") == \"Engineer\").collect()[0]\n",
    "    assert row.avg_salary == 70000\n",
    "\n",
    "\n",
    "def test_feature_engineering_education_level(base_df):\n",
    "    df = apply_feature_engineering(clean_data(base_df))\n",
    "    levels = {r.education_level for r in df.select(\"education_level\").collect()}\n",
    "    assert \"Bachelors\" in levels\n",
    "    assert \"Masters\" in levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. Pipeline Execution\n",
    "# ============================================================\n",
    "spark = None\n",
    "try:\n",
    "    print(\">>> Phase 1: Running Unit Tests...\")\n",
    "    ipytest.run()\n",
    "\n",
    "    spark = get_spark()\n",
    "    print(f\">>> Phase 2: Connected to Master. UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "    path = \"/dataset/nyc-jobs.csv\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"{path} not found. Check volume mapping.\")\n",
    "\n",
    "    raw_df = (\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(path)\n",
    "    )\n",
    "\n",
    "    df = apply_feature_engineering(\n",
    "        clean_data(\n",
    "            normalize_columns(raw_df)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    df.show()\n",
    "\n",
    "    stats, salary_distribution, corr, high_agency, avg_2yr, skills = compute_kpis(df)\n",
    "\n",
    "    stats.show(5)\n",
    "    corr.show()\n",
    "    high_agency.show(5)\n",
    "    avg_2yr.show(5)\n",
    "    skills.show(5)\n",
    "\n",
    "    pdf = stats.toPandas()\n",
    "    pdf.plot(kind=\"bar\", x=\"job_category\", y=\"job_count\", title=\"NYC Job Categories\")\n",
    "    plt.show()\n",
    "\n",
    "    df.unpersist()\n",
    "    print(\">>> Pipeline Finished Successfully\")\n",
    "\n",
    "finally:\n",
    "    if spark:\n",
    "        print(\">>> Stopping Spark session...\")\n",
    "        spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
